{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# TensorFlow 2 quickstart for experts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUNzJc4jTj6G"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/quickstart/advanced\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/quickstart/advanced.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiH7AC-NTniF"
      },
      "source": [
        "This is a [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) notebook file. Python programs are run directly in the browserâ€”a great way to learn and use TensorFlow. To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.\n",
        "\n",
        "1. In Colab, connect to a Python runtime: At the top-right of the menu bar, select *CONNECT*.\n",
        "2. Run all the notebook code cells: Select *Runtime* > *Run all*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOsVdx6GGHmU"
      },
      "source": [
        "Download and install TensorFlow 2. Import TensorFlow into your program:\n",
        "\n",
        "Note: Upgrade `pip` to install the TensorFlow 2 package. See the [install guide](https://www.tensorflow.org/install) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS7DDTiZGRTo"
      },
      "source": [
        "Import TensorFlow into your program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac82f6e-1836-4698-e707-3dea02d31a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "Load and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JqFRS6K07jJs"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Evqx0S22r_"
      },
      "source": [
        "Use `tf.data` to batch and shuffle the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Iu_quO024c2"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "Build the `tf.keras` model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGih-c2LgbJu"
      },
      "source": [
        "Choose an optimizer and loss function for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u48C9WQ774n4"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB6A1vcigsIe"
      },
      "source": [
        "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N0MqHFb4F_qn"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix4mEL65on-w"
      },
      "source": [
        "Use `tf.GradientTape` to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OZACiVqA8KQV"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8YT7UmFgpjV"
      },
      "source": [
        "Test the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xIKdEzHAJGt7"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-2pkctU_Ci7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fb36e6-e313-4f45-c7be-3c9efcc8a27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.14, Accuracy: 95.88, Test Loss: 0.07, Test Accuracy: 97.45\n",
            "Epoch 2, Loss: 0.04, Accuracy: 98.69, Test Loss: 0.05, Test Accuracy: 98.23\n",
            "Epoch 3, Loss: 0.02, Accuracy: 99.26, Test Loss: 0.06, Test Accuracy: 98.23\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_state()\n",
        "  train_accuracy.reset_state()\n",
        "  test_loss.reset_state()\n",
        "  test_accuracy.reset_state()\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result():0.2f}, '\n",
        "    f'Accuracy: {train_accuracy.result() * 100:0.2f}, '\n",
        "    f'Test Loss: {test_loss.result():0.2f}, '\n",
        "    f'Test Accuracy: {test_accuracy.result() * 100:0.2f}'\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. High level summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "v6Lho1W1C7cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Quick glance at the first few weight tensors\n",
        "for w in model.weights[:3]:\n",
        "    print(w.name, w.shape)"
      ],
      "metadata": {
        "id": "W5sYFFCrDLXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Zoom in on one layer\n",
        "dense1 = model.layers[0]        # index or use .get_layer(\"dense\") if you named it\n",
        "kernel, bias = dense1.get_weights()\n",
        "print(\"Kernel stats\", kernel.mean(), kernel.std())\n",
        "print(\"Bias stats\", bias.mean(), bias.std())"
      ],
      "metadata": {
        "id": "aI5cBWfHDVls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Zoom in on one layer\n",
        "dense1 = model.layers[0]        # index or use .get_layer(\"dense\") if you named it\n",
        "kernel, bias = dense1.get_weights()\n",
        "print(\"Kernel stats\", kernel.mean(), kernel.std())\n",
        "print(\"Bias stats\", bias.mean(), bias.std())"
      ],
      "metadata": {
        "id": "k6-QCgKoDcUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visual sanity check (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(kernel.flatten(), bins=50)\n",
        "plt.title(\"Weight distribution for dense1\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uavEyqh6Dkbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4JfEh7kvx6m"
      },
      "source": [
        "The image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adfa99a"
      },
      "source": [
        "# Get the input shape from a batch of images\n",
        "for images, labels in train_ds.take(1):\n",
        "  input_shape = images.shape\n",
        "  break\n",
        "\n",
        "# Create a dummy input tensor with the correct shape\n",
        "dummy_input = tf.zeros(input_shape)\n",
        "\n",
        "# Pass the dummy input through the first layer to get its output\n",
        "layer = model.layers[0]\n",
        "output_tensor = layer(dummy_input)\n",
        "\n",
        "# Now you can access the shape of the output tensor\n",
        "print(layer.name)\n",
        "print(output_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "245a5ced"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fd90a28"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze the sentiment of a sample text\n",
        "text = \"I love using Google Colab, it's so convenient!\"\n",
        "result = sentiment_analyzer(text)\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ae5a30e"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the Named Entity Recognition pipeline\n",
        "ner_recognizer = pipeline(\"ner\", grouped_entities=True)\n",
        "\n",
        "# Analyze a sample text for named entities\n",
        "text = \"Google was founded by Larry Page and Sergey Brin in 1998.\"\n",
        "results = ner_recognizer(text)\n",
        "\n",
        "# Print the results\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e0c3f04"
      },
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Use a newer, smaller model that is likely to work\n",
        "pipe = pipeline(\"text-generation\", model=\"distilgpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32100482"
      },
      "source": [
        "# Define a prompt\n",
        "prompt = \"Write a short story about a robot learning to feel.\"\n",
        "\n",
        "# Generate text using the loaded pipeline\n",
        "# You can adjust max_length to control the length of the generated text\n",
        "generated_text = pipe(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text[0]['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "246cfc22"
      },
      "source": [
        "!pip install pymupdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ddc066"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get the entity groups from the extracted entities\n",
        "entity_groups_2 = [entity['entity_group'] for entity in entities_2]\n",
        "\n",
        "# Count the occurrences of each entity group\n",
        "entity_group_counts_2 = Counter(entity_groups_2)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Entity group counts from second PDF:\")\n",
        "for group, count in entity_group_counts_2.most_common():\n",
        "    print(f\"{group}: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faa4d34a"
      },
      "source": [
        "# @title\n",
        "# Apply NER to the extracted text from the PDF\n",
        "# We'll process the text in chunks if it's very long\n",
        "chunk_size = 500 # Process in chunks of 500 characters\n",
        "entities = []\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i+chunk_size]\n",
        "    chunk_entities = ner_recognizer(chunk)\n",
        "    entities.extend(chunk_entities)\n",
        "\n",
        "# Print the first 20 identified entities as an example\n",
        "print(\"First 20 identified entities:\")\n",
        "for entity in entities[:20]:\n",
        "    print(entity)\n",
        "\n",
        "# You can explore the full 'entities' list to see all identified entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed54763e"
      },
      "source": [
        "!pip install networkx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60a27da9"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes with attributes (like a 'type' attribute)\n",
        "G.add_node(\"Python\", type=\"ProgrammingLanguage\")\n",
        "G.add_node(\"TensorFlow\", type=\"Library/Framework\")\n",
        "\n",
        "# Add an edge (relationship) between nodes with a 'relation' attribute\n",
        "G.add_edge(\"TensorFlow\", \"Python\", relation=\"USES\")\n",
        "\n",
        "# You can add more nodes and edges\n",
        "G.add_node(\"Keras\", type=\"Library/Framework\")\n",
        "G.add_edge(\"Keras\", \"TensorFlow\", relation=\"PART_OF\")\n",
        "\n",
        "# Print information about the graph\n",
        "print(\"Nodes:\", G.nodes(data=True)) # data=True shows node attributes\n",
        "print(\"Edges:\", G.edges(data=True)) # data=True shows edge attributes\n",
        "\n",
        "# Optional: Visualize the graph (requires matplotlib)\n",
        "# nx.draw(G, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62863dd2"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a new empty graph\n",
        "entity_graph = nx.Graph()\n",
        "\n",
        "# Iterate through the entities from the first PDF\n",
        "# Make sure the 'entities' variable is defined by running cell faa4d34a first!\n",
        "if 'entities' in locals():\n",
        "    for entity in entities:\n",
        "        # Use the entity word as the node ID\n",
        "        node_id = entity['word']\n",
        "        # Use the entity group as a node attribute\n",
        "        node_attributes = {'type': entity['entity_group']}\n",
        "\n",
        "        # Add the node to the graph\n",
        "        entity_graph.add_node(node_id, **node_attributes)\n",
        "\n",
        "    print(f\"Added {entity_graph.number_of_nodes()} nodes to the graph from the first PDF entities.\")\n",
        "    # Print the first few nodes to check\n",
        "    print(\"First 10 nodes:\", list(entity_graph.nodes(data=True))[:10])\n",
        "else:\n",
        "    print(\"Error: 'entities' variable not found. Please run cell faa4d34a first to extract entities from the first PDF.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef453f05"
      },
      "source": [
        "# Iterate through the entities from the second PDF\n",
        "# Make sure the 'entities_2' variable is defined by running cell 76eac752 first!\n",
        "if 'entities_2' in locals():\n",
        "    for entity in entities_2:\n",
        "        # Use the entity word as the node ID\n",
        "        node_id = entity['word']\n",
        "        # Use the entity group as a node attribute\n",
        "        node_attributes = {'type': entity['entity_group']}\n",
        "\n",
        "        # Add the node to the graph. NetworkX handles adding existing nodes gracefully.\n",
        "        entity_graph.add_node(node_id, **node_attributes)\n",
        "\n",
        "    print(f\"Added entities from the second PDF to the graph.\")\n",
        "    print(f\"Total number of nodes in the graph: {entity_graph.number_of_nodes()}\")\n",
        "    # Print the first few nodes again to check\n",
        "    print(\"First 10 nodes:\", list(entity_graph.nodes(data=True))[:10])\n",
        "else:\n",
        "    print(\"Error: 'entities_2' variable not found. Please run cell 76eac752 first to extract entities from the second PDF.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "advanced.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}